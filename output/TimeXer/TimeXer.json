{
    "structure": [
        {
            "type": "EnEmbedding",
            "params": {
                "training": "True",
                "patch_len": "16",
                "d_model": "128"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class EnEmbedding(nn.Module):\n    def __init__(self, n_vars, d_model, patch_len, dropout):\n        s...",
            "name": "en_embedding"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "16",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": false,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "en_embedding.value_embedding"
        },
        {
            "type": "PositionalEmbedding",
            "params": {
                "training": "True"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(P...",
            "name": "en_embedding.position_embedding"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "en_embedding.dropout"
        },
        {
            "type": "DataEmbedding_inverted",
            "params": {
                "training": "True"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class DataEmbedding_inverted(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', f...",
            "name": "ex_embedding"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "96",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "ex_embedding.value_embedding"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "ex_embedding.dropout"
        },
        {
            "type": "Encoder",
            "params": {
                "training": "True",
                "projection": "None"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Encoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        ...",
            "name": "encoder"
        },
        {
            "type": "ModuleList",
            "params": {
                "training": "True"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class ModuleList(Module):\n    r\"\"\"Holds submodules in a list.\n\n    :class:`~torch.nn.ModuleList` can...",
            "name": "encoder.layers"
        },
        {
            "type": "EncoderLayer",
            "params": {
                "training": "True",
                "activation": "<built-in function gelu>"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class EncoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff...",
            "name": "encoder.layers.0"
        },
        {
            "type": "AttentionLayer",
            "params": {
                "training": "True",
                "n_heads": "4"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n  ...",
            "name": "encoder.layers.0.self_attention"
        },
        {
            "type": "FullAttention",
            "params": {
                "training": "True",
                "scale": "None",
                "mask_flag": "False",
                "output_attention": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attenti...",
            "name": "encoder.layers.0.self_attention.inner_attention"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.0.self_attention.inner_attention.dropout"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.self_attention.query_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.self_attention.key_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.self_attention.value_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.self_attention.out_projection"
        },
        {
            "type": "AttentionLayer",
            "params": {
                "training": "True",
                "n_heads": "4"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n  ...",
            "name": "encoder.layers.0.cross_attention"
        },
        {
            "type": "FullAttention",
            "params": {
                "training": "True",
                "scale": "None",
                "mask_flag": "False",
                "output_attention": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attenti...",
            "name": "encoder.layers.0.cross_attention.inner_attention"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.0.cross_attention.inner_attention.dropout"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.cross_attention.query_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.cross_attention.key_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.cross_attention.value_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.0.cross_attention.out_projection"
        },
        {
            "type": "Conv1d",
            "params": {
                "training": "True",
                "in_channels": "128",
                "out_channels": "256",
                "transposed": "False",
                "groups": "1",
                "padding_mode": "zeros"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Conv1d(_ConvNd):\n    __doc__ = (\n        r\"\"\"Applies a 1D convolution over an input signal com...",
            "name": "encoder.layers.0.conv1"
        },
        {
            "type": "Conv1d",
            "params": {
                "training": "True",
                "in_channels": "256",
                "out_channels": "128",
                "transposed": "False",
                "groups": "1",
                "padding_mode": "zeros"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Conv1d(_ConvNd):\n    __doc__ = (\n        r\"\"\"Applies a 1D convolution over an input signal com...",
            "name": "encoder.layers.0.conv2"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.0.norm1"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.0.norm2"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.0.norm3"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.0.dropout"
        },
        {
            "type": "EncoderLayer",
            "params": {
                "training": "True",
                "activation": "<built-in function gelu>"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class EncoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff...",
            "name": "encoder.layers.1"
        },
        {
            "type": "AttentionLayer",
            "params": {
                "training": "True",
                "n_heads": "4"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n  ...",
            "name": "encoder.layers.1.self_attention"
        },
        {
            "type": "FullAttention",
            "params": {
                "training": "True",
                "scale": "None",
                "mask_flag": "False",
                "output_attention": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attenti...",
            "name": "encoder.layers.1.self_attention.inner_attention"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.1.self_attention.inner_attention.dropout"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.self_attention.query_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.self_attention.key_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.self_attention.value_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.self_attention.out_projection"
        },
        {
            "type": "AttentionLayer",
            "params": {
                "training": "True",
                "n_heads": "4"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n  ...",
            "name": "encoder.layers.1.cross_attention"
        },
        {
            "type": "FullAttention",
            "params": {
                "training": "True",
                "scale": "None",
                "mask_flag": "False",
                "output_attention": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attenti...",
            "name": "encoder.layers.1.cross_attention.inner_attention"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.1.cross_attention.inner_attention.dropout"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.cross_attention.query_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.cross_attention.key_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.cross_attention.value_projection"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "128",
                "out_features": "128"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "encoder.layers.1.cross_attention.out_projection"
        },
        {
            "type": "Conv1d",
            "params": {
                "training": "True",
                "in_channels": "128",
                "out_channels": "256",
                "transposed": "False",
                "groups": "1",
                "padding_mode": "zeros"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Conv1d(_ConvNd):\n    __doc__ = (\n        r\"\"\"Applies a 1D convolution over an input signal com...",
            "name": "encoder.layers.1.conv1"
        },
        {
            "type": "Conv1d",
            "params": {
                "training": "True",
                "in_channels": "256",
                "out_channels": "128",
                "transposed": "False",
                "groups": "1",
                "padding_mode": "zeros"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Conv1d(_ConvNd):\n    __doc__ = (\n        r\"\"\"Applies a 1D convolution over an input signal com...",
            "name": "encoder.layers.1.conv2"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.1.norm1"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.1.norm2"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.layers.1.norm3"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "encoder.layers.1.dropout"
        },
        {
            "type": "LayerNorm",
            "params": {
                "training": "True",
                "eps": "1e-05",
                "elementwise_affine": "True"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs.\n\n    This ...",
            "name": "encoder.norm"
        },
        {
            "type": "FlattenHead",
            "params": {
                "training": "True",
                "n_vars": "7"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class FlattenHead(nn.Module):\n    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n   ...",
            "name": "head"
        },
        {
            "type": "Flatten",
            "params": {
                "training": "True",
                "start_dim": "-2",
                "end_dim": "-1"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Flatten(Module):\n    r\"\"\"\n    Flattens a contiguous range of dims into a tensor.\n\n    For use ...",
            "name": "head.flatten"
        },
        {
            "type": "Linear",
            "params": {
                "training": "True",
                "in_features": "896",
                "out_features": "96"
            },
            "has_weight": true,
            "has_bias": true,
            "source": "class Linear(Module):\n    r\"\"\"Applies an affine linear transformation to the incoming data: :math:`y...",
            "name": "head.linear"
        },
        {
            "type": "Dropout",
            "params": {
                "training": "True",
                "p": "0.1",
                "inplace": "False"
            },
            "has_weight": false,
            "has_bias": false,
            "source": "class Dropout(_DropoutNd):\n    r\"\"\"During training, randomly zeroes some of the elements of the inpu...",
            "name": "head.dropout"
        }
    ],
    "connections": [
        {
            "source": "mean",
            "target": "detach",
            "type": "normal"
        },
        {
            "source": "detach",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "detach",
            "target": "<built-in function sub>",
            "type": "normal"
        },
        {
            "source": "<built-in function sub>",
            "target": "<built-in method var of type object at 0x00007FFAC3B57450>",
            "type": "normal"
        },
        {
            "source": "<built-in function sub>",
            "target": "<built-in function itruediv>",
            "type": "normal"
        },
        {
            "source": "<built-in method var of type object at 0x00007FFAC3B57450>",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "<built-in function add>",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "<built-in method sqrt of type object at 0x00007FFAC3B57450>",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "<built-in method sqrt of type object at 0x00007FFAC3B57450>",
            "target": "<built-in function itruediv>",
            "type": "normal"
        },
        {
            "source": "permute",
            "target": "L__model___head_flatten",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_glb_token",
            "target": "repeat",
            "type": "normal"
        },
        {
            "source": "repeat",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "unfold",
            "target": "size",
            "type": "normal"
        },
        {
            "source": "unfold",
            "target": "size",
            "type": "normal"
        },
        {
            "source": "unfold",
            "target": "<built-in method reshape of type object at 0x00007FFAC3B57450>",
            "type": "normal"
        },
        {
            "source": "<built-in function mul>",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_value_embedding",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_position_embedding_pe",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_dropout",
            "target": "L__model___encoder_layers_0_self_attention_key_projection",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_dropout",
            "target": "L__model___encoder_layers_0_self_attention_value_projection",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_dropout",
            "target": "L__model___encoder_layers_0_self_attention_query_projection",
            "type": "normal"
        },
        {
            "source": "L__model___en_embedding_dropout",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_0_cross_attention_key_projection",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_0_cross_attention_value_projection",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_1_cross_attention_key_projection",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_1_cross_attention_value_projection",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_0_self_attention_query_projection",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_dropout",
            "target": "L__model___encoder_layers_1_self_attention_query_projection",
            "type": "normal"
        },
        {
            "source": "view",
            "target": "L__model___encoder_layers_1_cross_attention_out_projection",
            "type": "normal"
        },
        {
            "source": "view",
            "target": "L__model___encoder_layers_1_self_attention_query_projection",
            "type": "normal"
        },
        {
            "source": "<function einsum at 0x000001690CB15E40>",
            "target": "contiguous",
            "type": "normal"
        },
        {
            "source": "<built-in method softmax of type object at 0x00007FFAC3B57450>",
            "target": "L__model___encoder_layers_1_cross_attention_inner_attention_dropout",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_self_attention_inner_attention_dropout",
            "target": "<function einsum at 0x000001690CB15E40>",
            "type": "normal"
        },
        {
            "source": "contiguous",
            "target": "L__model___encoder_layers_1_cross_attention_out_projection",
            "type": "skip"
        },
        {
            "source": "contiguous",
            "target": "L__model___encoder_layers_1_self_attention_query_projection",
            "type": "skip"
        },
        {
            "source": "contiguous",
            "target": "view",
            "type": "normal"
        },
        {
            "source": "unsqueeze",
            "target": "repeat",
            "type": "normal"
        },
        {
            "source": "<built-in method reshape of type object at 0x00007FFAC3B57450>",
            "target": "unsqueeze",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_dropout",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "<built-in method cat of type object at 0x00007FFAC3B57450>",
            "target": "transpose",
            "type": "normal"
        },
        {
            "source": "<built-in method cat of type object at 0x00007FFAC3B57450>",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "transpose",
            "target": "L__model___encoder_layers_1_dropout",
            "type": "normal"
        },
        {
            "source": "<built-in function gelu>",
            "target": "L__model___encoder_layers_1_dropout",
            "type": "normal"
        },
        {
            "source": "arg0",
            "target": "mean",
            "type": "normal"
        },
        {
            "source": "arg0",
            "target": "<built-in function sub>",
            "type": "normal"
        },
        {
            "source": "arg1",
            "target": "permute",
            "type": "normal"
        },
        {
            "source": "<built-in function itruediv>",
            "target": "permute",
            "type": "normal"
        },
        {
            "source": "<built-in function itruediv>",
            "target": "permute",
            "type": "normal"
        },
        {
            "source": "size",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___ex_embedding_value_embedding",
            "target": "L__model___ex_embedding_dropout",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_self_attention_query_projection",
            "target": "view",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm1",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm1",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_cross_attention_inner_attention_dropout",
            "target": "<function einsum at 0x000001690CB15E40>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm2",
            "target": "<built-in method cat of type object at 0x00007FFAC3B57450>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm3",
            "target": "L__model___encoder_layers_1_self_attention_key_projection",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm3",
            "target": "L__model___encoder_layers_1_self_attention_value_projection",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm3",
            "target": "L__model___encoder_layers_1_self_attention_query_projection",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_0_norm3",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_self_attention_query_projection",
            "target": "view",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_self_attention_inner_attention_dropout",
            "target": "<function einsum at 0x000001690CB15E40>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_norm1",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_norm1",
            "target": "<built-in function getitem>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_cross_attention_inner_attention_dropout",
            "target": "<function einsum at 0x000001690CB15E40>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_dropout",
            "target": "<built-in function add>",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_layers_1_norm3",
            "target": "L__model___encoder_norm",
            "type": "normal"
        },
        {
            "source": "L__model___encoder_norm",
            "target": "<built-in method reshape of type object at 0x00007FFAC3B57450>",
            "type": "normal"
        },
        {
            "source": "L__model___head_flatten",
            "target": "L__model___head_linear",
            "type": "normal"
        },
        {
            "source": "L__model___head_linear",
            "target": "L__model___head_dropout",
            "type": "normal"
        },
        {
            "source": "L__model___head_dropout",
            "target": "permute",
            "type": "normal"
        }
    ],
    "inputs": [
        "l_args_0_",
        "l_args_1_",
        "arg2",
        "arg3"
    ]
}